{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c379e83a",
   "metadata": {},
   "source": [
    "# <span style=\"color:red; font-weight:bold; \">A clean and modern RangeViT implementation for SemanticKITTI in PyTorch 2.4</span>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85e44a8",
   "metadata": {},
   "source": [
    "## <span style=\"font-weight:bold\">1. DataLoader</span>\n",
    "\n",
    "### 1.1 Dataset Structure\n",
    "The dataset should be structured as follows:\n",
    "```\n",
    "sequences/\n",
    "├── 00/\n",
    "│   ├── preprocess/\n",
    "│   │   ├── 000000.bin\n",
    "│   │   ├── 000001.bin\n",
    "├── 01/\n",
    "│   ├── preprocess/\n",
    "│   │   ├── 000000.bin\n",
    "│   │   ├── 000001.bin\n",
    "```\n",
    "\n",
    "Libraries required: timm, torch, tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8409f8-0faf-4b4f-84a0-04ac79c1757f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies in the requirements.txt file\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b73372e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from timm.models.vision_transformer import PatchEmbed\n",
    "\n",
    "from segmentation_models_pytorch.losses import FocalLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8a4b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Projection\n",
    "class ScanProjection(object):\n",
    "    '''\n",
    "    Project the 3D point cloud to 2D data with range projection\n",
    "\n",
    "    Adapted from A. Milioto et al. https://github.com/PRBonn/lidar-bonnetal\n",
    "    '''\n",
    "\n",
    "    def __init__(self, proj_w, proj_h):\n",
    "        # params of proj img size\n",
    "        self.proj_w = proj_w\n",
    "        self.proj_h = proj_h\n",
    "\n",
    "\n",
    "    def doProjection(self, pointcloud: np.ndarray):\n",
    "\n",
    "        # get depth of all points\n",
    "        depth = np.linalg.norm(pointcloud[:, :3], 2, axis=1)\n",
    "        # get point cloud components\n",
    "        x = pointcloud[:, 0]\n",
    "        y = pointcloud[:, 1]\n",
    "        z = pointcloud[:, 2]\n",
    "        # label is the last column of pointcloud\n",
    "        label = pointcloud[:,-1]\n",
    "        # remove the last column from pointcloud\n",
    "        pointcloud = pointcloud[:, :-1]\n",
    "        # remove flag, R, G, and B from pointcloud\n",
    "        pointcloud = pointcloud[:, :-4]  # now only has [x, y, z, intensity]\n",
    "        # get angles of all points\n",
    "        yaw = -np.arctan2(y, -x)\n",
    "        proj_x = 0.5 * (yaw / np.pi + 1.0)  # in [0.0, 1.0]\n",
    "        #breakpoint()\n",
    "        new_raw = np.nonzero((proj_x[1:] < 0.2) * (proj_x[:-1] > 0.8))[0] + 1\n",
    "        proj_y = np.zeros_like(proj_x)\n",
    "        proj_y[new_raw] = 1\n",
    "        proj_y = np.cumsum(proj_y)\n",
    "        # scale to image size using angular resolution\n",
    "        proj_x = proj_x * self.proj_w - 0.001\n",
    "\n",
    "        # round and clamp for use as index\n",
    "        proj_x = np.maximum(np.minimum(\n",
    "            self.proj_w - 1, np.floor(proj_x)), 0).astype(np.int32)\n",
    "        # wrap proj_x so if proj_x < 1024 it will be added 1024, if proj_x >= 1024 it will be subtracted 1024\n",
    "        proj_x = np.where(proj_x < 1024, proj_x + 1024, proj_x - 1024)\n",
    "\n",
    "        proj_y = np.maximum(np.minimum(\n",
    "            self.proj_h - 1, np.floor(proj_y)), 0).astype(np.int32)\n",
    "\n",
    "        # order in decreasing depth\n",
    "        indices = np.arange(depth.shape[0])\n",
    "        order = np.argsort(depth)[::-1]\n",
    "        depth = depth[order]\n",
    "        indices = indices[order]\n",
    "        pointcloud = pointcloud[order]\n",
    "        proj_y = proj_y[order]\n",
    "        proj_x = proj_x[order]\n",
    "        label = label[order]\n",
    "\n",
    "        # get projection result\n",
    "        proj_range = np.full((self.proj_h, self.proj_w), -1, dtype=np.float32)\n",
    "        proj_range[proj_y, proj_x] = depth\n",
    "\n",
    "        proj_pointcloud = np.full((self.proj_h, self.proj_w, pointcloud.shape[1]), -1, dtype=np.float32)\n",
    "        proj_pointcloud[proj_y, proj_x] = pointcloud\n",
    "\n",
    "        proj_idx = np.full((self.proj_h, self.proj_w), -1, dtype=np.int32)\n",
    "        proj_idx[proj_y, proj_x] = indices\n",
    "\n",
    "        proj_label = np.full((self.proj_h, self.proj_w), 0, dtype=np.int32)\n",
    "        proj_label[proj_y, proj_x] = label\n",
    "\n",
    "        # create proj_tensor with cascade proj_pointcloud and proj_range\n",
    "        # proj_pointcloud has size (64, 2048, 4)\n",
    "        # proj_range has size (64, 2048)\n",
    "        proj_pointcloud = proj_pointcloud[0:48, 790:1250, :]  # cut the pointcloud to [H, W, C]\n",
    "        proj_range =proj_range[0:48, 790:1250]  # cut the range to [H, W]\n",
    "        proj_label = proj_label[0:48, 790:1250]\n",
    "\n",
    "        proj_tensor = np.concatenate((proj_range[..., np.newaxis], proj_pointcloud), axis=-1) # [range, x, y, z, flag, R, G, B]\n",
    "        return proj_tensor, proj_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5203ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DataLoader\n",
    "class KITTISegmentationDataset(Dataset):\n",
    "    def __init__(self, root_dir, sequences):\n",
    "        self.root_dir = root_dir\n",
    "        self.file_list = []\n",
    "        for seq in sequences:\n",
    "            seq_dir = os.path.join(root_dir, seq)\n",
    "            assert os.path.exists(seq_dir), f\"Sequence {seq} does not exist in {root_dir}\"\n",
    "            file_list = []\n",
    "            pc_dir = os.path.join(seq_dir, 'preprocess')\n",
    "            # Get the list of files (full path) in the point cloud directory\n",
    "            file_list = [os.path.join(pc_dir, f) for f in os.listdir(pc_dir) if f.endswith('.bin')]\n",
    "            self.file_list.extend(file_list)\n",
    "        # Setup the projection parameters\n",
    "        self.projection = ScanProjection(proj_w=2048, proj_h=64)\n",
    "        # Define the learning map for semantic labels\n",
    "        # This map is used to convert the original labels to a smaller set of classes\n",
    "        self.learning_map = {0: 0, 1: 0, 10: 1, 11: 2, 13: 5, 15: 3, 16: 5, 18: 4, 20: 5,\n",
    "            30: 6, 31: 7, 32: 8, 40: 9, 44: 10, 48: 11, 49: 12, 50: 13,\n",
    "            51: 14, 52: 0, 60: 9, 70: 15, 71: 16, 72: 17, 80: 18, 81: 19,\n",
    "            99: 0, 252: 1, 253: 7, 254: 6, 255: 8, 256: 5, 257: 5, 258: 4, 259: 5}\n",
    "        # Create a mapping array with size large enough to cover the largest key\n",
    "        self.max_key = max(self.learning_map.keys())\n",
    "        self.map_array = np.zeros((self.max_key + 1,), dtype=np.int32)\n",
    "        # Fill the mapping array with the learning map values\n",
    "        for key, value in self.learning_map.items():\n",
    "            self.map_array[key] = value\n",
    "            \n",
    "    # Read the point cloud data from binary files\n",
    "    @staticmethod\n",
    "    def readPCD(path):\n",
    "        pcd = np.fromfile(path, dtype=np.float32).reshape(-1, 9) # 9 channels: x, y, z, intensity, flag, R, G, B, label\n",
    "        return pcd\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pc_path = self.file_list[idx]\n",
    "\n",
    "        # Load binary data\n",
    "        pc = self.readPCD(pc_path)  # x, y, z, intensity\n",
    "        img, label = self.projection.doProjection(pc) # shape [H, W, C]\n",
    "        # Map the labels using the learning map\n",
    "        label = self.map_array[label]  # map to smaller set of classes\n",
    "        img = torch.tensor(img).permute(2, 0, 1).float()  # to [C, H, W]\n",
    "        label = torch.tensor(label).long()                # [H, W]\n",
    "        # Normalize the tensor\n",
    "        mean = torch.tensor([12.12, 10.88, 0.23, -1.04, 0.21])\n",
    "        std = torch.tensor([12.32, 11.47, 6.91, 0.86, 0.16])\n",
    "        img = (img - mean[:, None, None]) / std[:, None, None]\n",
    "        return img, label\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace32aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardware to run on; Uncomment appropriate lines\n",
    "# runpod cloud RTX 4090: ~ 10 it/s: paralell might not needed since training takes about 3 hours.\n",
    "# Powerful GPU so increase the batch size for faster training, the num_workers also increase so that the data loading is not a bottleneck\n",
    "dataset = KITTISegmentationDataset('./dataset/sequences',['00','01','02','03','04','05','06','07','09','10'])\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=16)\n",
    "dataset_val = KITTISegmentationDataset('./dataset/sequences',['08'])\n",
    "loader_val = DataLoader(dataset_val, batch_size=16, shuffle=False, num_workers = 16)\n",
    "# local Legion computer: ~ 2 it/s\n",
    "# batch_size and num_workers are set to 1 due to limited resources\n",
    "# dataset = KITTISegmentationDataset('../SemanticKITTI/dataset/sequences',['00','01','02','03','04','05','06','07','09','10'])\n",
    "# loader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=1)\n",
    "# dataset_val = KITTISegmentationDataset('../SemanticKITTI/dataset/sequences',['08'])\n",
    "# loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b9da22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RangeViTSegmentationModel(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_height = 64\n",
    "        self.input_width = 2048\n",
    "        self.patch_height = 2\n",
    "        self.patch_width = 8\n",
    "\n",
    "        self.backbone = timm.create_model(\n",
    "            'vit_small_patch16_384',\n",
    "            pretrained=True,\n",
    "            in_chans=in_channels,\n",
    "            num_classes=0,\n",
    "            global_pool='',\n",
    "            features_only=False\n",
    "        )\n",
    "\n",
    "        # Override patch embedding\n",
    "        self.backbone.patch_embed = PatchEmbed(\n",
    "            img_size=(self.input_height, self.input_width),\n",
    "            patch_size=(self.patch_height, self.patch_width),\n",
    "            in_chans=in_channels,\n",
    "            embed_dim=self.backbone.embed_dim\n",
    "        )\n",
    "\n",
    "        self.grid_h, self.grid_w = self.backbone.patch_embed.grid_size  # (32, 256)\n",
    "        self.num_patches = self.grid_h * self.grid_w\n",
    "        print(f\"Grid size: {self.grid_h} x {self.grid_w}, Patches: {self.num_patches}\")\n",
    "\n",
    "        expected_tokens = 1 + self.num_patches\n",
    "        if self.backbone.pos_embed.shape[1] != expected_tokens:\n",
    "            self.update_pos_embed()\n",
    "\n",
    "        self.seg_head = nn.Sequential(\n",
    "            nn.Conv2d(self.backbone.embed_dim, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "        )\n",
    "\n",
    "        self.original_size = None\n",
    "\n",
    "    def update_pos_embed(self):\n",
    "        old_pos_embed = self.backbone.pos_embed\n",
    "        cls_token = old_pos_embed[:, :1, :]\n",
    "        patch_pos = old_pos_embed[:, 1:, :]\n",
    "\n",
    "        # Original pretrained ViT size was 24x24 : (384x384)/(16x16)\n",
    "        patch_pos = patch_pos.reshape(1, 24, 24, -1).permute(0, 3, 1, 2)\n",
    "        patch_pos = F.interpolate(patch_pos, size=(self.grid_h, self.grid_w), mode='bilinear', align_corners=False)\n",
    "        patch_pos = patch_pos.permute(0, 2, 3, 1).reshape(1, self.num_patches, -1)\n",
    "        new_pos_embed = torch.cat([cls_token, patch_pos], dim=1)\n",
    "        self.backbone.pos_embed = nn.Parameter(new_pos_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        self.original_size = x.shape[2:]  # Expect (64, 2048)\n",
    "\n",
    "        # DO NOT resize\n",
    "        feats = self.backbone(x)  # [B, 8193, C]\n",
    "        C = feats.shape[-1]\n",
    "        feats = feats[:, 1:, :].reshape(B, self.grid_h, self.grid_w, C).permute(0, 3, 1, 2)\n",
    "\n",
    "        logits = self.seg_head(feats)  # [B, num_classes, 32, 256]\n",
    "        logits = F.interpolate(logits, size=self.original_size, mode='bilinear', align_corners=False)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f224fafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(preds, labels, num_classes):\n",
    "    ious = []\n",
    "    correct = (preds == labels)\n",
    "    accuracy = correct.sum().float() / labels.numel()\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        # Get binary predictions and labels for this class\n",
    "        pred_cls = (preds == cls)\n",
    "        label_cls = (labels == cls)\n",
    "\n",
    "        # Intersection and Union\n",
    "        intersection = (pred_cls & label_cls).sum().float()\n",
    "        union = (pred_cls | label_cls).sum().float()\n",
    "\n",
    "        if union == 0:\n",
    "            ious.append(torch.tensor(float('nan'), device=device))  # undefined for this class\n",
    "        else:\n",
    "            ious.append(intersection / union)\n",
    "\n",
    "    # Mean IoU (excluding NaNs)\n",
    "    ious_tensor = torch.stack(ious)\n",
    "    mIoU = torch.nanmean(ious_tensor)\n",
    "\n",
    "    return mIoU, ious_tensor, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9d299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "num_classes = 20\n",
    "in_channels = 5 # range, x, y, z, intensity, flag, R, G, B\n",
    "num_epochs = 60\n",
    "model = RangeViTSegmentationModel(num_classes=num_classes, in_channels=in_channels).to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs with DataParallel!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model.to(device)\n",
    "# Create Focal Loss\n",
    "criterion = FocalLoss(mode='multiclass', ignore_index=0)  # Use your ignore_index if needed\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0004)\n",
    "# Load the model if you have a pre-trained one\n",
    "pretrain_path = 'range_vit_segmentation_noRGB_patch.pth'\n",
    "if os.path.exists(pretrain_path):\n",
    "    print(f\"Loading pre-trained model from {pretrain_path}\")\n",
    "    model.load_state_dict(torch.load(pretrain_path, map_location=device))\n",
    "# Training loop\n",
    "best_val_mIoU = 0.0\n",
    "model.train() # a switch that tells the model to be in training mode. It doesn't actually perform any training computations itself\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "    batch_bar = tqdm(loader, desc=f\"Training Epoch {epoch+1}\", leave=False)\n",
    "    average_loss = 0.0\n",
    "    average_acc = 0.0\n",
    "    average_mIoU = 0.0\n",
    "    for imgs, labels in batch_bar:\n",
    "        imgs = imgs.to(device)                # [B, C, H, W]\n",
    "        labels = labels.to(device)             # [B, H, W]\n",
    "        optimizer.zero_grad()\n",
    "        # actually perform the training step\n",
    "        outputs = model(imgs)                 # [B, num_classes, H, W]\n",
    "        loss = criterion(outputs, labels)     # Compute raw loss\n",
    "\n",
    "        preds = outputs.argmax(dim=1)         # [B, H, W]\n",
    "        mIoU, ious, acc = compute_iou(preds, labels, num_classes) \n",
    "        loss.backward()  # Calculates gradients of the loss with respect to all model parameters\n",
    "        optimizer.step() # Updates Parameter \n",
    "        batch_bar.set_postfix(loss=loss.item(), mIoU=mIoU.item(), acc=acc.item())\n",
    "        average_loss += loss.item()\n",
    "        average_acc += acc.item()\n",
    "        average_mIoU += mIoU.item()\n",
    "        \n",
    "    print(f\"Epoch [{epoch+1}] Train Loss: {average_loss/len(loader):.4f}, Train mIoU: {average_mIoU/len(loader):.4f}, Train Acc: {average_acc/len(loader):.4f}\")\n",
    "\n",
    "    model.eval()  # <-- switch to eval mode\n",
    "    with torch.no_grad():  # turn off gradient tracking for speed and memory\n",
    "        average_loss = 0.0\n",
    "        average_acc = 0.0\n",
    "        average_mIoU = 0.0\n",
    "        batch_bar = tqdm(loader_val, desc=f\"Evaluating\", leave=False)\n",
    "        for imgs, labels in batch_bar:\n",
    "    \n",
    "            imgs = imgs.to(device)                # [B, C, H, W]\n",
    "            labels = labels.to(device)             # [B, H, W]\n",
    "    \n",
    "            outputs = model(imgs)                 # [B, num_classes, H, W]\n",
    "            loss = criterion(outputs, labels)     # Compute raw loss\n",
    "    \n",
    "            preds = outputs.argmax(dim=1)         # [B, H, W]\n",
    "            mIoU, ious, acc = compute_iou(preds, labels, num_classes) \n",
    "            batch_bar.set_postfix(loss=loss.item(), mIoU=mIoU.item(), acc=acc.item())\n",
    "            average_loss += loss.item()\n",
    "            average_acc += acc.item()\n",
    "            average_mIoU += mIoU.item()\n",
    "            \n",
    "        print(f\"Validation Loss: {average_loss/len(loader_val):.4f}, Validation mIoU: {average_mIoU/len(loader_val):.4f}, Validation Acc: {average_acc/len(loader_val):.4f}\")\n",
    "        val_mIoU = average_mIoU/len(loader_val)\n",
    "        if val_mIoU > best_val_mIoU:\n",
    "            best_val_mIoU = val_mIoU\n",
    "            print('saving better model...')\n",
    "            torch.save(model.state_dict(), pretrain_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pykitti",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
