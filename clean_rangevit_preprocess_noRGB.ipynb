{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c379e83a",
   "metadata": {},
   "source": [
    "# <span style=\"color:red; font-weight:bold; \">A clean and modern RangeViT implementation for SemanticKITTI in PyTorch 2.4</span>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85e44a8",
   "metadata": {},
   "source": [
    "## <span style=\"font-weight:bold\">1. DataLoader</span>\n",
    "\n",
    "### 1.1 Dataset Structure\n",
    "The dataset should be structured as follows:\n",
    "```\n",
    "sequences/\n",
    "├── 00/\n",
    "│   ├── preprocess/\n",
    "│   │   ├── 000000.bin\n",
    "│   │   ├── 000001.bin\n",
    "├── 01/\n",
    "│   ├── preprocess/\n",
    "│   │   ├── 000000.bin\n",
    "│   │   ├── 000001.bin\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b73372e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8a4b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Projection\n",
    "class ScanProjection(object):\n",
    "    '''\n",
    "    Project the 3D point cloud to 2D data with range projection\n",
    "\n",
    "    Adapted from A. Milioto et al. https://github.com/PRBonn/lidar-bonnetal\n",
    "    '''\n",
    "\n",
    "    def __init__(self, proj_w, proj_h):\n",
    "        # params of proj img size\n",
    "        self.proj_w = proj_w\n",
    "        self.proj_h = proj_h\n",
    "\n",
    "\n",
    "    def doProjection(self, pointcloud: np.ndarray):\n",
    "\n",
    "        # get depth of all points\n",
    "        depth = np.linalg.norm(pointcloud[:, :3], 2, axis=1)\n",
    "        # get point cloud components\n",
    "        x = pointcloud[:, 0]\n",
    "        y = pointcloud[:, 1]\n",
    "        z = pointcloud[:, 2]\n",
    "        # label is the last column of pointcloud\n",
    "        label = pointcloud[:,-1]\n",
    "        # remove the last column from pointcloud\n",
    "        pointcloud = pointcloud[:, :-1]\n",
    "        # remove flag, R, G, and B from pointcloud\n",
    "        pointcloud = pointcloud[:, :-4]  # now only has [x, y, z, intensity]\n",
    "        # get angles of all points\n",
    "        yaw = -np.arctan2(y, -x)\n",
    "        proj_x = 0.5 * (yaw / np.pi + 1.0)  # in [0.0, 1.0]\n",
    "        #breakpoint()\n",
    "        new_raw = np.nonzero((proj_x[1:] < 0.2) * (proj_x[:-1] > 0.8))[0] + 1\n",
    "        proj_y = np.zeros_like(proj_x)\n",
    "        proj_y[new_raw] = 1\n",
    "        proj_y = np.cumsum(proj_y)\n",
    "        # scale to image size using angular resolution\n",
    "        proj_x = proj_x * self.proj_w - 0.001\n",
    "\n",
    "        # round and clamp for use as index\n",
    "        proj_x = np.maximum(np.minimum(\n",
    "            self.proj_w - 1, np.floor(proj_x)), 0).astype(np.int32)\n",
    "\n",
    "        proj_y = np.maximum(np.minimum(\n",
    "            self.proj_h - 1, np.floor(proj_y)), 0).astype(np.int32)\n",
    "\n",
    "        # order in decreasing depth\n",
    "        indices = np.arange(depth.shape[0])\n",
    "        order = np.argsort(depth)[::-1]\n",
    "        depth = depth[order]\n",
    "        indices = indices[order]\n",
    "        pointcloud = pointcloud[order]\n",
    "        proj_y = proj_y[order]\n",
    "        proj_x = proj_x[order]\n",
    "        label = label[order]\n",
    "\n",
    "        # get projection result\n",
    "        proj_range = np.full((self.proj_h, self.proj_w), -1, dtype=np.float32)\n",
    "        proj_range[proj_y, proj_x] = depth\n",
    "\n",
    "        proj_pointcloud = np.full((self.proj_h, self.proj_w, pointcloud.shape[1]), -1, dtype=np.float32)\n",
    "        proj_pointcloud[proj_y, proj_x] = pointcloud\n",
    "\n",
    "        proj_idx = np.full((self.proj_h, self.proj_w), -1, dtype=np.int32)\n",
    "        proj_idx[proj_y, proj_x] = indices\n",
    "\n",
    "        proj_label = np.full((self.proj_h, self.proj_w), 0, dtype=np.int32)\n",
    "        proj_label[proj_y, proj_x] = label\n",
    "\n",
    "        # create proj_tensor with cascade proj_pointcloud and proj_range\n",
    "        # proj_pointcloud has size (64, 2048, 4)\n",
    "        # proj_range has size (64, 2048)\n",
    "        proj_tensor = np.concatenate((proj_range[..., np.newaxis], proj_pointcloud), axis=-1) # [range, x, y, z, flag, R, G, B]\n",
    "        return proj_tensor, proj_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5203ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DataLoader\n",
    "class KITTISegmentationDataset(Dataset):\n",
    "    def __init__(self, root_dir, sequences):\n",
    "        self.root_dir = root_dir\n",
    "        self.file_list = []\n",
    "        for seq in sequences:\n",
    "            seq_dir = os.path.join(root_dir, seq)\n",
    "            assert os.path.exists(seq_dir), f\"Sequence {seq} does not exist in {root_dir}\"\n",
    "            file_list = []\n",
    "            pc_dir = os.path.join(seq_dir, 'preprocess')\n",
    "            # Get the list of files (full path) in the point cloud directory\n",
    "            file_list = [os.path.join(pc_dir, f) for f in os.listdir(pc_dir) if f.endswith('.bin')]\n",
    "            self.file_list.extend(file_list)\n",
    "        # Setup the projection parameters\n",
    "        self.projection = ScanProjection(proj_w=2048, proj_h=64)\n",
    "        # Define the learning map for semantic labels\n",
    "        # This map is used to convert the original labels to a smaller set of classes\n",
    "        self.learning_map = {0: 0, 1: 0, 10: 1, 11: 2, 13: 5, 15: 3, 16: 5, 18: 4, 20: 5,\n",
    "            30: 6, 31: 7, 32: 8, 40: 9, 44: 10, 48: 11, 49: 12, 50: 13,\n",
    "            51: 14, 52: 0, 60: 9, 70: 15, 71: 16, 72: 17, 80: 18, 81: 19,\n",
    "            99: 0, 252: 1, 253: 7, 254: 6, 255: 8, 256: 5, 257: 5, 258: 4, 259: 5}\n",
    "        # Create a mapping array with size large enough to cover the largest key\n",
    "        self.max_key = max(self.learning_map.keys())\n",
    "        self.map_array = np.zeros((self.max_key + 1,), dtype=np.int32)\n",
    "        # Fill the mapping array with the learning map values\n",
    "        for key, value in self.learning_map.items():\n",
    "            self.map_array[key] = value\n",
    "            \n",
    "    # Read the point cloud data from binary files\n",
    "    @staticmethod\n",
    "    def readPCD(path):\n",
    "        pcd = np.fromfile(path, dtype=np.float32).reshape(-1, 9) # 9 channels: x, y, z, intensity, flag, R, G, B, label\n",
    "        return pcd\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pc_path = self.file_list[idx]\n",
    "\n",
    "        # Load binary data\n",
    "        pc = self.readPCD(pc_path)  # x, y, z, intensity\n",
    "        img, label = self.projection.doProjection(pc) # shape [H, W, C]\n",
    "        # Map the labels using the learning map\n",
    "        label = self.map_array[label]  # map to smaller set of classes\n",
    "        img = torch.tensor(img).permute(2, 0, 1).float()  # to [C, H, W]\n",
    "        label = torch.tensor(label).long()                # [H, W]\n",
    "\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace32aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = KITTISegmentationDataset('../SemanticKITTI/dataset/sequences',['03','04'])\n",
    "loader = DataLoader(dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58cd3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RangeViTSegmentationModel(nn.Module):\n",
    "    def __init__(self, in_channels, n_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create ViT model without features_only to see what we actually get\n",
    "        self.backbone = timm.create_model(\n",
    "            'vit_small_patch16_384',       \n",
    "            pretrained=True,\n",
    "            in_chans=in_channels,\n",
    "            num_classes=0,  # Set num_classes to 0 to avoid classification head; this has no effect on number of classes in seg_head (still 20)\n",
    "            global_pool='', # disables CLS token pooling\n",
    "            features_only=False  # Don't use features_only\n",
    "        )\n",
    "        \n",
    "        # Get the actual feature dimension from the model\n",
    "        feat_dim = 384  # This should be 384 for vit_small\n",
    "        hidden_dim = 256\n",
    "        # Print for debugging\n",
    "        print(f\"ViT feature dimension: {feat_dim}\")\n",
    "        print(f\"number of classes: {n_classes}\")\n",
    "        # Create segmentation head with the correct input dimension\n",
    "        self.seg_head = nn.Sequential(\n",
    "            nn.Conv2d(feat_dim, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_dim, n_classes, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "        self.original_size = None  # Store original size for resizing back\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Store original size for later upsampling\n",
    "        self.original_size = x.shape[2:]\n",
    "        # Resize input to 384x384 (what ViT expects)\n",
    "        x_resized = F.interpolate(x, size=(384, 384), mode='bilinear', align_corners=False)\n",
    "        # Extract features from backbone\n",
    "        feats = self.backbone(x_resized)\n",
    "        # Reshape features for segmentation head \n",
    "        # ViT returns tokens, we need to reshape to 2D feature map\n",
    "        B = x_resized.shape[0]\n",
    "        h = w = int(384 / 16)  # 16 is patch size of vit_small_patch16_384\n",
    "        C = feats.shape[-1]\n",
    "        # Remove CLS token and reshape to [B, C, h, w]\n",
    "        feats = feats[:, 1:, :].reshape(B, h, w, C).permute(0, 3, 1, 2)\n",
    "        \n",
    "        # Apply segmentation head\n",
    "        logits = self.seg_head(feats)\n",
    "        \n",
    "        # Resize back to original dimensions\n",
    "        return F.interpolate(logits, size=self.original_size, mode='bilinear', align_corners=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f224fafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(preds, labels, num_classes):\n",
    "    ious = []\n",
    "    correct = (preds == labels)\n",
    "    accuracy = correct.sum() / labels.numel()\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        # Get binary predictions and labels for this class\n",
    "        pred_cls = (preds == cls)\n",
    "        label_cls = (labels == cls)\n",
    "\n",
    "        # Intersection and Union\n",
    "        intersection = (pred_cls & label_cls).sum().float()\n",
    "        union = (pred_cls | label_cls).sum().float()\n",
    "\n",
    "        if union == 0:\n",
    "            ious.append(torch.tensor(float('nan')))  # undefined for this class\n",
    "        else:\n",
    "            ious.append(intersection / union)\n",
    "\n",
    "    # Mean IoU (excluding NaNs)\n",
    "    ious_tensor = torch.stack(ious)\n",
    "    mIoU = torch.nanmean(ious_tensor)\n",
    "\n",
    "    return mIoU, ious_tensor, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9d299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = 20\n",
    "in_channels = 5 # range, x, y, z, intensity, flag, R, G, B\n",
    "num_epochs = 10\n",
    "model = RangeViTSegmentationModel(n_classes=num_classes, in_channels=in_channels).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "# Load the model if you have a pre-trained one\n",
    "pretrain_path = 'range_vit_segmentation_noRGB.pth'\n",
    "if os.path.exists(pretrain_path):\n",
    "    print(f\"Loading pre-trained model from {pretrain_path}\")\n",
    "    model.load_state_dict(torch.load(pretrain_path, map_location=device))\n",
    "# Training loop\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "    model.train() # a switch that tells the model to be in training mode. It doesn't actually perform any training computations itself\n",
    "    batch_bar = tqdm(loader, desc=f\"Training Epoch {epoch+1}\", leave=False)\n",
    "    for imgs, labels in batch_bar:\n",
    "        imgs = imgs.to(device)                # [B, C, H, W]\n",
    "        labels = labels.to(device)             # [B, H, W]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # actually perform the training step\n",
    "        outputs = model(imgs)                 # [B, num_classes, H, W]\n",
    "        loss = criterion(outputs, labels)     # Compute raw loss\n",
    "\n",
    "        preds = outputs.argmax(dim=1)         # [B, H, W]\n",
    "        mIoU, ious, acc = compute_iou(preds, labels, num_classes) \n",
    "        loss.backward()  # Calculates gradients of the loss with respect to all model parameters\n",
    "        optimizer.step() # Updates Parameter \n",
    "        batch_bar.set_postfix(loss=loss.item(), mIoU=mIoU.item(), acc=acc.item())\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}] Loss: {loss.item():.4f}, mIoU: {mIoU.item():.4f}, Acc: {acc.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795f44c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), pretrain_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddbb692",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val = KITTISegmentationDataset('../SemanticKITTI/dataset/sequences',['08'])\n",
    "loader_val = DataLoader(dataset_val, batch_size=1, shuffle=True)\n",
    "model.eval()  # <-- switch to eval mode\n",
    "with torch.no_grad():  # turn off gradient tracking for speed and memory\n",
    "    for imgs, labels in loader_val:\n",
    "        valid_mask = (imgs[:, 5, :, :] > 0)  # Assuming the fifth channel is flag, and we want to ignore invalid points\n",
    "        valid_mask = valid_mask.to(device)\n",
    "        imgs = imgs.to(device)                # [B, C, H, W]\n",
    "        labels = labels.to(device)             # [B, H, W]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # actually perform the training step\n",
    "        imgs = imgs * valid_mask.unsqueeze(1)              # [B, C, H, W]\n",
    "        outputs = model(imgs)                 # [B, num_classes, H, W]\n",
    "        loss = criterion(outputs, labels)     # Compute raw loss\n",
    "        # Only keep valid entries\n",
    "        masked_loss = loss * valid_mask\n",
    "        loss = masked_loss.sum() / valid_mask.sum().clamp(min=1)\n",
    "\n",
    "        preds = outputs.argmax(dim=1)         # [B, H, W]\n",
    "        mIoU, ious = compute_iou(preds, labels, num_classes, valid_mask) \n",
    "\n",
    "mIoU, ious, acc = compute_iou(preds, labels, num_classes, valid_mask)\n",
    "print(f\"Validation Loss: {loss.item():.4f}, mIoU: {mIoU:.4f}, acc: {acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pykitti",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
