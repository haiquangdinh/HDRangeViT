{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c379e83a",
   "metadata": {},
   "source": [
    "# <span style=\"color:red; font-weight:bold; \">A clean and modern RangeViT implementation for SemanticKITTI in PyTorch 2.4</span>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85e44a8",
   "metadata": {},
   "source": [
    "## <span style=\"font-weight:bold\">1. DataLoader</span>\n",
    "\n",
    "### 1.1 Dataset Structure\n",
    "The dataset should be structured as follows:\n",
    "```\n",
    "sequences/\n",
    "├── 00/\n",
    "│   ├── preprocess/\n",
    "│   │   ├── 000000.bin\n",
    "│   │   ├── 000001.bin\n",
    "├── 01/\n",
    "│   ├── preprocess/\n",
    "│   │   ├── 000000.bin\n",
    "│   │   ├── 000001.bin\n",
    "```\n",
    "\n",
    "Libraries required: timm, torch, tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8409f8-0faf-4b4f-84a0-04ac79c1757f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone the repository branch WebVersion\n",
    "# !git clone -b WebVersion https://github.com/haiquangdinh/HDRangeViT.git\n",
    "# setup the git email and name\n",
    "# !git config --global user.email \"haiquangdinh@gmail.com\"\n",
    "# !git config --global user.name \"Hai Dinh\"\n",
    "# commit the changes\n",
    "# !git add . && git commit -m \"Message\" && git push origin WebVersion\n",
    "\n",
    "# install dependencies in the requirements.txt file\n",
    "# !pip install -r requirements.txt\n",
    "\n",
    "# reload the module after making changes\n",
    "# import importlib\n",
    "# importlib.reload(mymodule)\n",
    "\n",
    "# Set a flag to indicate where the code is running\n",
    "# Set to True if running on RunPod, False if running locally\n",
    "is_runpod = False  \n",
    "\n",
    "# Reload modules automatically\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b73372e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from segmentation_models_pytorch.losses import FocalLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8a4b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from KITTISegmentationDataset import KITTISegmentationDataset\n",
    "from RangeViTSegmentationModel import RangeViTSegmentationModel\n",
    "from Evaluation import compute_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace32aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardware to run on; Uncomment appropriate lines\n",
    "\n",
    "# runpod cloud RTX 4090: ~ 10 it/s: paralell might not needed since training takes about 3 hours.\n",
    "# Powerful GPU so increase the batch size for faster training, the num_workers also increase so that the data loading is not a bottleneck\n",
    "# dataset = KITTISegmentationDataset('./dataset/sequences',['00','01','02','03','04','05','06','07','09','10'])\n",
    "# Till I see the overfit issue resolve, let only train on 00 and 01\n",
    "if is_runpod:\n",
    "    dataset = KITTISegmentationDataset('../dataset/sequences',['00','01'])\n",
    "    loader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=16)\n",
    "    dataset_val = KITTISegmentationDataset('../dataset/sequences',['08'])\n",
    "    loader_val = DataLoader(dataset_val, batch_size=16, shuffle=False, num_workers=16)\n",
    "    pretrain_path = '../range_vit_segmentation_noRGB_patch.pth'\n",
    "\n",
    "else:\n",
    "    # local Legion computer: ~ 2 it/s\n",
    "    # batch_size and num_workers are set to 1 due to limited resources\n",
    "    # dataset = KITTISegmentationDataset('../SemanticKITTI/dataset/sequences',['00','01','02','03','04','05','06','07','09','10'])\n",
    "    dataset = KITTISegmentationDataset('../SemanticKITTI/dataset/sequences',['00'])\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=1)\n",
    "    dataset_val = KITTISegmentationDataset('../SemanticKITTI/dataset/sequences',['08'])\n",
    "    loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, num_workers=1)\n",
    "    pretrain_path = '../range_vit_segmentation_noRGB_patch.pth'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9d299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "num_classes = 20\n",
    "in_channels = 5 # range, x, y, z, intensity, flag, R, G, B\n",
    "num_epochs = 60\n",
    "model = RangeViTSegmentationModel(num_classes=num_classes, in_channels=in_channels).to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs with DataParallel!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model.to(device)\n",
    "# Create Focal Loss\n",
    "criterion = FocalLoss(mode='multiclass', ignore_index=0)  # Use your ignore_index if needed\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0004)\n",
    "# Load the model if you have a pre-trained one\n",
    "if os.path.exists(pretrain_path):\n",
    "    print(f\"Loading pre-trained model from {pretrain_path}\")\n",
    "    model.load_state_dict(torch.load(pretrain_path, map_location=device))\n",
    "# Training loop\n",
    "best_val_mIoU = 0.0\n",
    "model.train() # a switch that tells the model to be in training mode. It doesn't actually perform any training computations itself\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "    batch_bar = tqdm(loader, desc=f\"Training Epoch {epoch+1}\", leave=False)\n",
    "    average_loss = 0.0\n",
    "    average_acc = 0.0\n",
    "    average_mIoU = 0.0\n",
    "    for imgs, labels in batch_bar:\n",
    "        imgs = imgs.to(device)                # [B, C, H, W]\n",
    "        labels = labels.to(device)             # [B, H, W]\n",
    "        optimizer.zero_grad()\n",
    "        # actually perform the training step\n",
    "        outputs = model(imgs)                 # [B, num_classes, H, W]\n",
    "        loss = criterion(outputs, labels)     # Compute raw loss\n",
    "\n",
    "        preds = outputs.argmax(dim=1)         # [B, H, W]\n",
    "        mIoU, ious, acc = compute_iou(preds, labels, num_classes) \n",
    "        loss.backward()  # Calculates gradients of the loss with respect to all model parameters\n",
    "        optimizer.step() # Updates Parameter \n",
    "        batch_bar.set_postfix(loss=loss.item(), mIoU=mIoU.item(), acc=acc.item())\n",
    "        average_loss += loss.item()\n",
    "        average_acc += acc.item()\n",
    "        average_mIoU += mIoU.item()\n",
    "        \n",
    "    print(f\"Epoch [{epoch+1}] Train Loss: {average_loss/len(loader):.4f}, Train mIoU: {average_mIoU/len(loader):.4f}, Train Acc: {average_acc/len(loader):.4f}\")\n",
    "\n",
    "    model.eval()  # <-- switch to eval mode\n",
    "    with torch.no_grad():  # turn off gradient tracking for speed and memory\n",
    "        average_loss = 0.0\n",
    "        average_acc = 0.0\n",
    "        average_mIoU = 0.0\n",
    "        batch_bar = tqdm(loader_val, desc=f\"Evaluating\", leave=False)\n",
    "        for imgs, labels in batch_bar:\n",
    "    \n",
    "            imgs = imgs.to(device)                # [B, C, H, W]\n",
    "            labels = labels.to(device)             # [B, H, W]\n",
    "    \n",
    "            outputs = model(imgs)                 # [B, num_classes, H, W]\n",
    "            loss = criterion(outputs, labels)     # Compute raw loss\n",
    "    \n",
    "            preds = outputs.argmax(dim=1)         # [B, H, W]\n",
    "            mIoU, ious, acc = compute_iou(preds, labels, num_classes) \n",
    "            batch_bar.set_postfix(loss=loss.item(), mIoU=mIoU.item(), acc=acc.item())\n",
    "            average_loss += loss.item()\n",
    "            average_acc += acc.item()\n",
    "            average_mIoU += mIoU.item()\n",
    "            \n",
    "        print(f\"Validation Loss: {average_loss/len(loader_val):.4f}, Validation mIoU: {average_mIoU/len(loader_val):.4f}, Validation Acc: {average_acc/len(loader_val):.4f}\")\n",
    "        val_mIoU = average_mIoU/len(loader_val)\n",
    "        if val_mIoU > best_val_mIoU:\n",
    "            best_val_mIoU = val_mIoU\n",
    "            print('saving better model...')\n",
    "            torch.save(model.state_dict(), pretrain_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pykitti",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
