{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c379e83a",
   "metadata": {},
   "source": [
    "# <span style=\"color:red; font-weight:bold; \">A clean and modern RangeViT implementation for SemanticKITTI in PyTorch 2.4</span>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85e44a8",
   "metadata": {},
   "source": [
    "## <span style=\"font-weight:bold\">1. DataLoader</span>\n",
    "\n",
    "### 1.1 Dataset Structure\n",
    "The dataset should be structured as follows:\n",
    "```\n",
    "sequences/\n",
    "├── 00/\n",
    "│   ├── preprocess/\n",
    "│   │   ├── 000000.bin\n",
    "│   │   ├── 000001.bin\n",
    "├── 01/\n",
    "│   ├── preprocess/\n",
    "│   │   ├── 000000.bin\n",
    "│   │   ├── 000001.bin\n",
    "```\n",
    "\n",
    "Libraries required: timm, torch, tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b73372e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8a4b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Projection\n",
    "class ScanProjection(object):\n",
    "    '''\n",
    "    Project the 3D point cloud to 2D data with range projection\n",
    "\n",
    "    Adapted from A. Milioto et al. https://github.com/PRBonn/lidar-bonnetal\n",
    "    '''\n",
    "\n",
    "    def __init__(self, proj_w, proj_h):\n",
    "        # params of proj img size\n",
    "        self.proj_w = proj_w\n",
    "        self.proj_h = proj_h\n",
    "\n",
    "\n",
    "    def doProjection(self, pointcloud: np.ndarray):\n",
    "\n",
    "        # get depth of all points\n",
    "        depth = np.linalg.norm(pointcloud[:, :3], 2, axis=1)\n",
    "        # get point cloud components\n",
    "        x = pointcloud[:, 0]\n",
    "        y = pointcloud[:, 1]\n",
    "        z = pointcloud[:, 2]\n",
    "        # label is the last column of pointcloud\n",
    "        label = pointcloud[:,-1]\n",
    "        # remove the last column from pointcloud\n",
    "        pointcloud = pointcloud[:, :-1]\n",
    "        # remove flag, R, G, and B from pointcloud\n",
    "        pointcloud = pointcloud[:, :-4]  # now only has [x, y, z, intensity]\n",
    "        # get angles of all points\n",
    "        yaw = -np.arctan2(y, -x)\n",
    "        proj_x = 0.5 * (yaw / np.pi + 1.0)  # in [0.0, 1.0]\n",
    "        #breakpoint()\n",
    "        new_raw = np.nonzero((proj_x[1:] < 0.2) * (proj_x[:-1] > 0.8))[0] + 1\n",
    "        proj_y = np.zeros_like(proj_x)\n",
    "        proj_y[new_raw] = 1\n",
    "        proj_y = np.cumsum(proj_y)\n",
    "        # scale to image size using angular resolution\n",
    "        proj_x = proj_x * self.proj_w - 0.001\n",
    "\n",
    "        # round and clamp for use as index\n",
    "        proj_x = np.maximum(np.minimum(\n",
    "            self.proj_w - 1, np.floor(proj_x)), 0).astype(np.int32)\n",
    "\n",
    "        proj_y = np.maximum(np.minimum(\n",
    "            self.proj_h - 1, np.floor(proj_y)), 0).astype(np.int32)\n",
    "\n",
    "        # order in decreasing depth\n",
    "        indices = np.arange(depth.shape[0])\n",
    "        order = np.argsort(depth)[::-1]\n",
    "        depth = depth[order]\n",
    "        indices = indices[order]\n",
    "        pointcloud = pointcloud[order]\n",
    "        proj_y = proj_y[order]\n",
    "        proj_x = proj_x[order]\n",
    "        label = label[order]\n",
    "\n",
    "        # get projection result\n",
    "        proj_range = np.full((self.proj_h, self.proj_w), -1, dtype=np.float32)\n",
    "        proj_range[proj_y, proj_x] = depth\n",
    "\n",
    "        proj_pointcloud = np.full((self.proj_h, self.proj_w, pointcloud.shape[1]), -1, dtype=np.float32)\n",
    "        proj_pointcloud[proj_y, proj_x] = pointcloud\n",
    "\n",
    "        proj_idx = np.full((self.proj_h, self.proj_w), -1, dtype=np.int32)\n",
    "        proj_idx[proj_y, proj_x] = indices\n",
    "\n",
    "        proj_label = np.full((self.proj_h, self.proj_w), 0, dtype=np.int32)\n",
    "        proj_label[proj_y, proj_x] = label\n",
    "\n",
    "        # create proj_tensor with cascade proj_pointcloud and proj_range\n",
    "        # proj_pointcloud has size (64, 2048, 4)\n",
    "        # proj_range has size (64, 2048)\n",
    "        proj_tensor = np.concatenate((proj_range[..., np.newaxis], proj_pointcloud), axis=-1) # [range, x, y, z, flag, R, G, B]\n",
    "        return proj_tensor, proj_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5203ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DataLoader\n",
    "class KITTISegmentationDataset(Dataset):\n",
    "    def __init__(self, root_dir, sequences):\n",
    "        self.root_dir = root_dir\n",
    "        self.file_list = []\n",
    "        for seq in sequences:\n",
    "            seq_dir = os.path.join(root_dir, seq)\n",
    "            assert os.path.exists(seq_dir), f\"Sequence {seq} does not exist in {root_dir}\"\n",
    "            file_list = []\n",
    "            pc_dir = os.path.join(seq_dir, 'preprocess')\n",
    "            # Get the list of files (full path) in the point cloud directory\n",
    "            file_list = [os.path.join(pc_dir, f) for f in os.listdir(pc_dir) if f.endswith('.bin')]\n",
    "            self.file_list.extend(file_list)\n",
    "        # Setup the projection parameters\n",
    "        self.projection = ScanProjection(proj_w=2048, proj_h=64)\n",
    "        # Define the learning map for semantic labels\n",
    "        # This map is used to convert the original labels to a smaller set of classes\n",
    "        self.learning_map = {0: 0, 1: 0, 10: 1, 11: 2, 13: 5, 15: 3, 16: 5, 18: 4, 20: 5,\n",
    "            30: 6, 31: 7, 32: 8, 40: 9, 44: 10, 48: 11, 49: 12, 50: 13,\n",
    "            51: 14, 52: 0, 60: 9, 70: 15, 71: 16, 72: 17, 80: 18, 81: 19,\n",
    "            99: 0, 252: 1, 253: 7, 254: 6, 255: 8, 256: 5, 257: 5, 258: 4, 259: 5}\n",
    "        # Create a mapping array with size large enough to cover the largest key\n",
    "        self.max_key = max(self.learning_map.keys())\n",
    "        self.map_array = np.zeros((self.max_key + 1,), dtype=np.int32)\n",
    "        # Fill the mapping array with the learning map values\n",
    "        for key, value in self.learning_map.items():\n",
    "            self.map_array[key] = value\n",
    "            \n",
    "    # Read the point cloud data from binary files\n",
    "    @staticmethod\n",
    "    def readPCD(path):\n",
    "        pcd = np.fromfile(path, dtype=np.float32).reshape(-1, 9) # 9 channels: x, y, z, intensity, flag, R, G, B, label\n",
    "        return pcd\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pc_path = self.file_list[idx]\n",
    "\n",
    "        # Load binary data\n",
    "        pc = self.readPCD(pc_path)  # x, y, z, intensity\n",
    "        img, label = self.projection.doProjection(pc) # shape [H, W, C]\n",
    "        # Map the labels using the learning map\n",
    "        label = self.map_array[label]  # map to smaller set of classes\n",
    "        img = torch.tensor(img).permute(2, 0, 1).float()  # to [C, H, W]\n",
    "        label = torch.tensor(label).long()                # [H, W]\n",
    "        # Normalize the tensor\n",
    "        mean = torch.tensor([12.12, 10.88, 0.23, -1.04, 0.21])\n",
    "        std = torch.tensor([12.32, 11.47, 6.91, 0.86, 0.16])\n",
    "        img = (img - mean[:, None, None]) / std[:, None, None]\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace32aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardware to run on; Uncomment appropriate lines\n",
    "# runpod cloud RTX 4090: ~ 10 it/s: paralell might not needed since training takes about 3 hours.\n",
    "# Powerful GPU so increase the batch size for faster training, the num_workers also increase so that the data loading is not a bottleneck\n",
    "# dataset = KITTISegmentationDataset('./dataset/sequences',['00','01','02','03','04','05','06','07','09','10'])\n",
    "# loader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=16)\n",
    "# dataset_val = KITTISegmentationDataset('./dataset/sequences',['08'])\n",
    "# loader_val = DataLoader(dataset_val, batch_size=16, shuffle=True, num_workers = 16)\n",
    "# local Legion computer: ~ 2 it/s\n",
    "# batch_size and num_workers are set to 1 due to limited resources\n",
    "dataset = KITTISegmentationDataset('../SemanticKITTI/dataset/sequences',['00','01','02','03','04','05','06','07','09','10'])\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=1)\n",
    "dataset_val = KITTISegmentationDataset('../SemanticKITTI/dataset/sequences',['08'])\n",
    "loader_val = DataLoader(dataset_val, batch_size=1, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58cd3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RangeViTSegmentationModel(nn.Module):\n",
    "    def __init__(self, in_channels, n_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create ViT model without features_only to see what we actually get\n",
    "        self.backbone = timm.create_model(\n",
    "            'vit_small_patch16_384',       \n",
    "            pretrained=True,\n",
    "            in_chans=in_channels,\n",
    "            num_classes=0,  # Set num_classes to 0 to avoid classification head; this has no effect on number of classes in seg_head (still 20)\n",
    "            global_pool='', # disables CLS token pooling\n",
    "            features_only=False  # Don't use features_only\n",
    "        )\n",
    "        \n",
    "        # Get the actual feature dimension from the model\n",
    "        feat_dim = 384  # This should be 384 for vit_small\n",
    "        hidden_dim = 256\n",
    "        # Print for debugging\n",
    "        print(f\"ViT feature dimension: {feat_dim}\")\n",
    "        print(f\"number of classes: {n_classes}\")\n",
    "        # Create segmentation head with the correct input dimension\n",
    "        self.seg_head = nn.Sequential(\n",
    "            nn.Conv2d(feat_dim, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_dim, n_classes, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "        self.original_size = None  # Store original size for resizing back\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Store original size for later upsampling\n",
    "        self.original_size = x.shape[2:]\n",
    "        # Resize input to 384x384 (what ViT expects)\n",
    "        x_resized = F.interpolate(x, size=(384, 384), mode='bilinear', align_corners=False)\n",
    "        # Extract features from backbone\n",
    "        feats = self.backbone(x_resized)\n",
    "        # Reshape features for segmentation head \n",
    "        # ViT returns tokens, we need to reshape to 2D feature map\n",
    "        B = x_resized.shape[0]\n",
    "        h = w = int(384 / 16)  # 16 is patch size of vit_small_patch16_384\n",
    "        C = feats.shape[-1]\n",
    "        # Remove CLS token and reshape to [B, C, h, w]\n",
    "        feats = feats[:, 1:, :].reshape(B, h, w, C).permute(0, 3, 1, 2)\n",
    "        \n",
    "        # Apply segmentation head\n",
    "        logits = self.seg_head(feats)\n",
    "        \n",
    "        # Resize back to original dimensions\n",
    "        upsampled_logits = F.interpolate(logits, size=self.original_size, mode='bilinear', align_corners=False)\n",
    "        return upsampled_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f224fafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(preds, labels, num_classes):\n",
    "    ious = []\n",
    "    correct = (preds == labels)\n",
    "    accuracy = correct.sum().float() / labels.numel()\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        # Get binary predictions and labels for this class\n",
    "        pred_cls = (preds == cls)\n",
    "        label_cls = (labels == cls)\n",
    "\n",
    "        # Intersection and Union\n",
    "        intersection = (pred_cls & label_cls).sum().float()\n",
    "        union = (pred_cls | label_cls).sum().float()\n",
    "\n",
    "        if union == 0:\n",
    "            ious.append(torch.tensor(float('nan'), device=device))  # undefined for this class\n",
    "        else:\n",
    "            ious.append(intersection / union)\n",
    "\n",
    "    # Mean IoU (excluding NaNs)\n",
    "    ious_tensor = torch.stack(ious)\n",
    "    mIoU = torch.nanmean(ious_tensor)\n",
    "\n",
    "    return mIoU, ious_tensor, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9d299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "num_classes = 20\n",
    "in_channels = 5 # range, x, y, z, intensity, flag, R, G, B\n",
    "num_epochs = 60\n",
    "model = RangeViTSegmentationModel(n_classes=num_classes, in_channels=in_channels).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "# Load the model if you have a pre-trained one\n",
    "pretrain_path = 'range_vit_segmentation_noRGB.pth'\n",
    "if os.path.exists(pretrain_path):\n",
    "    print(f\"Loading pre-trained model from {pretrain_path}\")\n",
    "    model.load_state_dict(torch.load(pretrain_path, map_location=device))\n",
    "# Training loop\n",
    "best_val_mIoU = 0.0\n",
    "model.train() # a switch that tells the model to be in training mode. It doesn't actually perform any training computations itself\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "    batch_bar = tqdm(loader, desc=f\"Training Epoch {epoch+1}\", leave=False)\n",
    "    average_loss = 0.0\n",
    "    average_acc = 0.0\n",
    "    average_mIoU = 0.0\n",
    "    for imgs, labels in batch_bar:\n",
    "        imgs = imgs.to(device)                # [B, C, H, W]\n",
    "        labels = labels.to(device)             # [B, H, W]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # actually perform the training step\n",
    "        outputs = model(imgs)                 # [B, num_classes, H, W]\n",
    "        loss = criterion(outputs, labels)     # Compute raw loss\n",
    "\n",
    "        preds = outputs.argmax(dim=1)         # [B, H, W]\n",
    "        mIoU, ious, acc = compute_iou(preds, labels, num_classes) \n",
    "        loss.backward()  # Calculates gradients of the loss with respect to all model parameters\n",
    "        optimizer.step() # Updates Parameter \n",
    "        batch_bar.set_postfix(loss=loss.item(), mIoU=mIoU.item(), acc=acc.item())\n",
    "        average_loss += loss.item()\n",
    "        average_acc += acc.item()\n",
    "        average_mIoU += mIoU.item()\n",
    "        \n",
    "    print(f\"Epoch [{epoch+1}] Train Loss: {average_loss/len(loader):.4f}, Train mIoU: {average_mIoU/len(loader):.4f}, Train Acc: {average_acc/len(loader):.4f}\")\n",
    "\n",
    "    model.eval()  # <-- switch to eval mode\n",
    "    with torch.no_grad():  # turn off gradient tracking for speed and memory\n",
    "        average_loss = 0.0\n",
    "        average_acc = 0.0\n",
    "        average_mIoU = 0.0\n",
    "        batch_bar = tqdm(loader_val, desc=f\"Evaluating\", leave=False)\n",
    "        for imgs, labels in batch_bar:\n",
    "    \n",
    "            imgs = imgs.to(device)                # [B, C, H, W]\n",
    "            labels = labels.to(device)             # [B, H, W]\n",
    "    \n",
    "            outputs = model(imgs)                 # [B, num_classes, H, W]\n",
    "            loss = criterion(outputs, labels)     # Compute raw loss\n",
    "    \n",
    "            preds = outputs.argmax(dim=1)         # [B, H, W]\n",
    "            mIoU, ious, acc = compute_iou(preds, labels, num_classes) \n",
    "            batch_bar.set_postfix(loss=loss.item(), mIoU=mIoU.item(), acc=acc.item())\n",
    "            average_loss += loss.item()\n",
    "            average_acc += acc.item()\n",
    "            average_mIoU += mIoU.item()\n",
    "            \n",
    "        print(f\"Validation Loss: {average_loss/len(loader_val):.4f}, Validation mIoU: {average_mIoU/len(loader_val):.4f}, Validation Acc: {average_acc/len(loader_val):.4f}\")\n",
    "        val_mIoU = average_mIoU/len(loader_val)\n",
    "        if val_mIoU > best_val_mIoU:\n",
    "            best_val_mIoU = val_mIoU\n",
    "            print('saving better model...')\n",
    "            torch.save(model.state_dict(), pretrain_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pykitti",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
