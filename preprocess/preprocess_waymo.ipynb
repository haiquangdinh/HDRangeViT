{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c65c5bd0",
   "metadata": {},
   "source": [
    "### tag name:\n",
    "\n",
    "camera_box, camera_calibration, camera_hkp, camera_image, camera_segmentation, camera_to_lidar_box_association\n",
    "\n",
    "lidar, lidar_box, lidar_calibration, lidar_camera_projection, lidar_camera_synced_box, lidar_hkp, lidar_pose, lidar_segmentation\n",
    "\n",
    "projected_lidar_box, stats, vehicle_pose\n",
    "\n",
    "#### lidar\n",
    "[LiDARComponent].range_image_return1\n",
    "\n",
    "[64, 2650, 4]\n",
    "\n",
    "channel 0: range (see spherical coordinate system definition)\n",
    "\n",
    "channel 1: lidar intensity\n",
    "\n",
    "channel 2: lidar elongation\n",
    "\n",
    "channel 3: is_in_nlz (1 = in, -1 = not in)\n",
    "\n",
    "#### lidar_camera_projection\n",
    "[LiDARCameraProjectionComponent].range_image_return1\n",
    "\n",
    "[6, 2650, 6]\n",
    "\n",
    "channel 0: camera name\n",
    "\n",
    "channel 1: x (axis along image width)\n",
    "\n",
    "channel 2: y (axis along image height)\n",
    "\n",
    "channel 3: camera name of 2nd projection (set to UNKNOWN if no projection)\n",
    "\n",
    "channel 4: x (axis along image width)\n",
    "\n",
    "channel 5: y (axis along image height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff871a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import tensorflow as tf\n",
    "from waymo_open_dataset.utils import range_image_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91eea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "WAYMO_TO_SEMKITTI_20 = {\n",
    "    0: 0,   # UNKNOWN -> unlabeled\n",
    "    1: 1,   # CAR -> car\n",
    "    2: 4,   # TRUCK -> truck\n",
    "    3: 4,   # BUS -> truck\n",
    "    4: 5,   # OTHER_VEHICLE -> other-vehicle\n",
    "    5: 8,   # MOTORCYCLIST -> motorcyclist\n",
    "    6: 7,   # BICYCLIST -> bicyclist\n",
    "    7: 6,   # PEDESTRIAN -> person\n",
    "    8: 19,  # SIGN -> traffic-sign\n",
    "    9: 19,  # TRAFFIC_LIGHT -> traffic-sign\n",
    "    10: 18, # POLE -> pole\n",
    "    11: 18, # CONSTRUCTION_CONE -> pole (approx)\n",
    "    12: 2,  # BICYCLE -> bicycle\n",
    "    13: 3,  # MOTORCYCLE -> motorcycle\n",
    "    14: 13, # BUILDING -> building\n",
    "    15: 15, # VEGETATION -> vegetation\n",
    "    16: 16, # TREE_TRUNK -> trunk\n",
    "    17: 12, # CURB -> other-ground (alt: road=9)\n",
    "    18: 9,  # ROAD -> road\n",
    "    19: 9,  # LANE_MARKER -> road\n",
    "    20: 12, # OTHER_GROUND -> other-ground\n",
    "    21: 11, # WALKABLE -> sidewalk\n",
    "    22: 11, # SIDEWALK -> sidewalk\n",
    "}\n",
    "\n",
    "def decode_jpeg_image(img_proto):\n",
    "    \"\"\"Decode a Waymo camera image proto into uint8 HxWx3 (RGB).\"\"\"\n",
    "    img = tf.image.decode_jpeg(img_proto, channels=3)\n",
    "    return img.numpy()\n",
    "\n",
    "# def decode_jpeg_image(img_proto):\n",
    "#     img_array = np.frombuffer(img_proto, np.uint8)\n",
    "#     img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)  # Returns BGR\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)       # Convert to RGB\n",
    "#     return img\n",
    "\n",
    "def _sample_rgb_nearest(img, rows, cols):\n",
    "    h, w = img.shape[:2]\n",
    "    rows = np.clip(rows, 0, h - 1)\n",
    "    cols = np.clip(cols, 0, w - 1)\n",
    "    return img[rows, cols, :]\n",
    "\n",
    "def colorize_range_image_v2(\n",
    "    proj,                 # [H, W, 6] = [cam1,row1,col1, cam2,row2,col2]\n",
    "    camera_images,        # dict: {camera_id:int -> np.uint8 [Hc,Wc,3]}\n",
    "    default_color=(128,128,128)\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      colors_hw3: uint8 [H, W, 3]\n",
    "      used_mask:  bool  [H, W]  (True where a camera projection was used)\n",
    "    \"\"\"\n",
    "    H, W, C = proj.shape\n",
    "    assert C == 6, \"Expected v2 projection with two triplets: [cam1,row1,col1, cam2,row2,col2]\"\n",
    "    colors = np.empty((H, W, 3), dtype=np.uint8)\n",
    "    colors[:] = np.array(default_color, dtype=np.uint8)\n",
    "    used = np.zeros((H, W), dtype=bool)\n",
    "\n",
    "    cam1 = proj[..., 0]; r1 = proj[..., 1]; c1 = proj[..., 2]\n",
    "    cam2 = proj[..., 3]; r2 = proj[..., 4]; c2 = proj[..., 5]\n",
    "    # SWAP row and col here!\n",
    "    r1, c1 = c1, r1\n",
    "    r2, c2 = c2, r2\n",
    "    # Pass 1: try primary projection per camera\n",
    "    for cid, img in camera_images.items():\n",
    "        m1 = (cam1 == cid)\n",
    "        if not np.any(m1):\n",
    "            continue\n",
    "        rr = r1[m1].astype(np.int32)\n",
    "        cc = c1[m1].astype(np.int32)\n",
    "        rgb = _sample_rgb_nearest(img, rr, cc)\n",
    "        ii, jj = np.nonzero(m1)\n",
    "        colors[ii, jj, :] = rgb\n",
    "        used[ii, jj] = True\n",
    "\n",
    "    # Pass 2: fallback to secondary where primary failed\n",
    "    need = ~used\n",
    "    for cid, img in camera_images.items():\n",
    "        m2 = (cam2 == cid) & need\n",
    "        if not np.any(m2):\n",
    "            continue\n",
    "        rr = r2[m2].astype(np.int32)\n",
    "        cc = c2[m2].astype(np.int32)\n",
    "        rgb = _sample_rgb_nearest(img, rr, cc)\n",
    "        ii, jj = np.nonzero(m2)\n",
    "        colors[ii, jj, :] = rgb\n",
    "        used[ii, jj] = True\n",
    "\n",
    "    return colors, used\n",
    "\n",
    "def colorize_waymo(label_channel, lidar_channel, extrinsic, beam_inclination, lidar_camera_projection, cam_images):\n",
    "    # Get the semantic ID (ignore instance ID)\n",
    "    semantic_id = label_channel[:,:,1]\n",
    "    # Convert orig_sem_label to sem_label using the map\n",
    "    sem_label = np.vectorize(WAYMO_TO_SEMKITTI_20.get)(semantic_id)\n",
    "    # Get x, y, z from lidar_channel\n",
    "    lidar_range = lidar_channel[:,:,0]\n",
    "    lidar_intensity = lidar_channel[:,:,1]\n",
    "\n",
    "    # #########################################################\n",
    "    # convert lidar_range, extrinsic, and beam_inclination to tensors with [B, W, H] with B=1\n",
    "    lidar_range = tf.convert_to_tensor(lidar_range, dtype=tf.float32)\n",
    "    lidar_range = tf.expand_dims(lidar_range, axis=0)\n",
    "    extrinsic = tf.convert_to_tensor(extrinsic, dtype=tf.float32)\n",
    "    extrinsic = tf.expand_dims(extrinsic, axis=0)\n",
    "    beam_inclination = tf.convert_to_tensor(beam_inclination, dtype=tf.float32)\n",
    "    beam_inclination = tf.expand_dims(beam_inclination, axis=0)\n",
    "    # Convert to 3D points (vehicle frame)\n",
    "    points = range_image_utils.extract_point_cloud_from_range_image(lidar_range, extrinsic, beam_inclination)\n",
    "    #  make point to by numarray\n",
    "    points_np = points.numpy()\n",
    "    # squeeze to remove dimensions of size 1\n",
    "    points_np = points_np.squeeze()\n",
    "    lidar_range = lidar_range.numpy().squeeze()\n",
    "    # sanity check points_np: if lidar_range = -1, then points_np = -1\n",
    "    points_np[lidar_range == -1] = 0\n",
    "    #  put the range image so that it has size {H, W} with five channel: range, x, y, z, intensity in such order\n",
    "    # lidar_raw[:,:,0] then points_np then lidar_raw[:,:,1]\n",
    "    points_final = np.zeros((64, 2650, 5))\n",
    "    points_final[:,:,0] = lidar_range\n",
    "    points_final[:,:,1:4] = points_np\n",
    "    points_final[:,:,4] = lidar_intensity\n",
    "    # now we have data that similar to that of the paper. Just need to add R,G, B (which is huge thing to do)\n",
    "    rgb_grid, used_mask = colorize_range_image_v2(lidar_camera_projection, cam_images, default_color=(0,0,0))\n",
    "    # combine the final points with range, x, y, z, intensity, flag, R, G, B and label\n",
    "    # flag is valid when and only when range > 0 and used_mask is one\n",
    "    points_final_output = np.zeros((64, 2650, 10))\n",
    "    points_final_output[:,:,0] = points_final[:,:,0]  # range\n",
    "    points_final_output[:,:,1] = points_final[:,:,1]  # x\n",
    "    points_final_output[:,:,2] = points_final[:,:,2]  # y\n",
    "    points_final_output[:,:,3] = points_final[:,:,3]  # z\n",
    "    points_final_output[:,:,4] = points_final[:,:,4]  # intensity\n",
    "    points_final_output[:,:,5] = (points_final[:,:,0] > 0) & (used_mask)  # flag\n",
    "    points_final_output[:,:,6] = rgb_grid[:,:, 0]  # R\n",
    "    points_final_output[:,:,7] = rgb_grid[:,:, 1]  # G\n",
    "    points_final_output[:,:,8] = rgb_grid[:,:, 2]  # B\n",
    "    points_final_output[:,:,9] = sem_label  # label (not used)\n",
    "\n",
    "    # sanity put: any -1 element is set to 0\n",
    "    points_final_output[points_final_output == -1] = 0\n",
    "\n",
    "    return points_final_output  # [H, W, 10]: range, x, y, z, intensity, flag, R, G, B, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223d8d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory with all components\n",
    "dataset_dir = '../../WoD/training' \n",
    "height = 64\n",
    "width = 2650\n",
    "preprocess_dir = f'{dataset_dir}/preprocess'\n",
    "os.makedirs(preprocess_dir, exist_ok=True)\n",
    "# search all .parquet files in the lidar_segmentation directory\n",
    "lidar_segmentation_dir = f'{dataset_dir}/lidar_segmentation'\n",
    "context_names = sorted([f for f in os.listdir(lidar_segmentation_dir) if f.endswith('.parquet')])\n",
    "# go through the list of context_names, make sure corresponding files exist in lidar, camera_image, lidar_calibration, and lidar_camera_projection\n",
    "for context_name in context_names:\n",
    "    lidar_path = f'{dataset_dir}/lidar/{context_name}'\n",
    "    camera_path = f'{dataset_dir}/camera_image/{context_name}'\n",
    "    lidar_calibration_path = f'{dataset_dir}/lidar_calibration/{context_name}'\n",
    "    lidar_camera_projection_path = f'{dataset_dir}/lidar_camera_projection/{context_name}'\n",
    "    if not (os.path.exists(lidar_path) and os.path.exists(camera_path) and os.path.exists(lidar_calibration_path) and os.path.exists(lidar_camera_projection_path)):\n",
    "        print(f'Missing files for context: {context_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2760c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the index of the context name \"6303332643743862144_5600_000_5620_000\"\n",
    "### In case it stop in the middle\n",
    "# context_name = \"6303332643743862144_5600_000_5620_000.parquet\"\n",
    "# context_index = -1\n",
    "# for i, context in enumerate(context_names):\n",
    "#     if context == context_name:\n",
    "#         context_index = i\n",
    "#         print(f'Found context at index: {context_index}')\n",
    "#         break\n",
    "# # remove all item before this index\n",
    "# context_names = context_names[context_index:]\n",
    "# print(f'Processing {len(context_names)} contexts starting from index {context_index}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003da0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lidar_segmentation_df = pd.read_parquet(f'{dataset_dir}/lidar_segmentation/{context_name}')\n",
    "# lidar_df = pd.read_parquet(f'{dataset_dir}/lidar/{context_name}')\n",
    "# camera_df = pd.read_parquet(f'{dataset_dir}/camera_image/{context_name}')\n",
    "# lidar_calibration_df = pd.read_parquet(f'{dataset_dir}/lidar_calibration/{context_name}')\n",
    "# lidar_camera_projection_df = pd.read_parquet(f'{dataset_dir}/lidar_camera_projection/{context_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3dc564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for frame_index, frame_data in tqdm(lidar_segmentation_df.iterrows(), desc=\"Processing Frames\"):\n",
    "#     timestamp = frame_data.get('key.frame_timestamp_micros', None)\n",
    "#     label_channel = lidar_segmentation_df[lidar_segmentation_df['key.frame_timestamp_micros'] == timestamp]['[LiDARSegmentationLabelComponent].range_image_return1.values']\n",
    "#     label_channel = np.array(label_channel.values[0]).reshape(height, width, 2) \n",
    "#     lidar_channel = lidar_df[(lidar_df['key.frame_timestamp_micros'] == timestamp) & (lidar_df['key.laser_name'] == 1)]['[LiDARComponent].range_image_return1.values']\n",
    "#     lidar_channel = np.array(lidar_channel.values[0]).reshape(height, width, 4) \n",
    "#     lidar_calibration = lidar_calibration_df[lidar_calibration_df['key.laser_name'] == 1]\n",
    "#     extrinsic = lidar_calibration['[LiDARCalibrationComponent].extrinsic.transform'].values[0].reshape(4, 4)\n",
    "#     beam_inclination = lidar_calibration['[LiDARCalibrationComponent].beam_inclination.values'].values[0]\n",
    "#     lidar_camera_projection = lidar_camera_projection_df[(lidar_camera_projection_df['key.frame_timestamp_micros'] == timestamp) & (lidar_camera_projection_df['key.laser_name'] == 1)]['[LiDARCameraProjectionComponent].range_image_return1.values']\n",
    "#     lidar_camera_projection = np.array(lidar_camera_projection.values[0]).reshape(height, width, 6)\n",
    "#     cameras = camera_df[camera_df['key.frame_timestamp_micros'] == timestamp]\n",
    "#     # go through each row in cameras, decode the image with [CameraImageComponent].image and key.camera_name (not that camera ID = key.camera_name - 1)\n",
    "#     cam_images = {}\n",
    "#     for index, row in cameras.iterrows():\n",
    "#         cam_images[row['key.camera_name']] = decode_jpeg_image(row['[CameraImageComponent].image'])\n",
    "#     pcd = colorize_waymo(label_channel, lidar_channel, extrinsic, beam_inclination, lidar_camera_projection, cam_images)\n",
    "#     # save pcd to file\n",
    "#     output_file = os.path.join(preprocess_dir, context_name.replace('.parquet', f'_{timestamp}.bin'))\n",
    "#     pcd.astype(np.float32).tofile(output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb86d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "for context_name in tqdm(context_names, desc=\"Processing Contexts\"):\n",
    "    # Read the components\n",
    "    lidar_segmentation_df = pd.read_parquet(f'{dataset_dir}/lidar_segmentation/{context_name}')\n",
    "    lidar_df = pd.read_parquet(f'{dataset_dir}/lidar/{context_name}')\n",
    "    camera_df = pd.read_parquet(f'{dataset_dir}/camera_image/{context_name}')\n",
    "    lidar_calibration_df = pd.read_parquet(f'{dataset_dir}/lidar_calibration/{context_name}')\n",
    "    lidar_camera_projection_df = pd.read_parquet(f'{dataset_dir}/lidar_camera_projection/{context_name}')\n",
    "    # lidar calibration is per context\n",
    "    lidar_calibration = lidar_calibration_df[lidar_calibration_df['key.laser_name'] == 1]\n",
    "    extrinsic = lidar_calibration['[LiDARCalibrationComponent].extrinsic.transform'].values[0].reshape(4, 4)\n",
    "    beam_inclination = lidar_calibration['[LiDARCalibrationComponent].beam_inclination.values'].values[0]\n",
    "\n",
    "    # Go through each frame in the context\n",
    "    for frame_index, frame_data in tqdm(lidar_segmentation_df.iterrows(), desc=\"Processing Frames\"):\n",
    "        timestamp = frame_data.get('key.frame_timestamp_micros', None)\n",
    "        # from now on:\n",
    "        # label from lidar_segmentation\n",
    "        # lidar data from lidar and lidar_camera_projection\n",
    "        # camera data from camera\n",
    "        # all filtered by timestamp\n",
    "        label_channel = lidar_segmentation_df[lidar_segmentation_df['key.frame_timestamp_micros'] == timestamp]['[LiDARSegmentationLabelComponent].range_image_return1.values']\n",
    "        label_channel = np.array(label_channel.values[0]).reshape(height, width, 2)  \n",
    "        lidar_channel = lidar_df[(lidar_df['key.frame_timestamp_micros'] == timestamp) & (lidar_df['key.laser_name'] == 1)]['[LiDARComponent].range_image_return1.values']\n",
    "        lidar_channel = np.array(lidar_channel.values[0]).reshape(height, width, 4)  \n",
    "        lidar_camera_projection = lidar_camera_projection_df[(lidar_camera_projection_df['key.frame_timestamp_micros'] == timestamp) & (lidar_camera_projection_df['key.laser_name'] == 1)]['[LiDARCameraProjectionComponent].range_image_return1.values']\n",
    "        lidar_camera_projection = np.array(lidar_camera_projection.values[0]).reshape(height, width, 6)\n",
    "        cameras = camera_df[camera_df['key.frame_timestamp_micros'] == timestamp]\n",
    "        # go through each row in cameras, decode the image with [CameraImageComponent].image and key.camera_name\n",
    "        cam_images = {}\n",
    "        for index, row in cameras.iterrows():\n",
    "            cam_images[row['key.camera_name']] = decode_jpeg_image(row['[CameraImageComponent].image'])\n",
    "        pcd = colorize_waymo(label_channel, lidar_channel, extrinsic, beam_inclination, lidar_camera_projection, cam_images)\n",
    "        # save pcd to file\n",
    "        output_file = os.path.join(preprocess_dir, context_name.replace('.parquet', f'_{timestamp}.bin'))\n",
    "        pcd.astype(np.float32).tofile(output_file)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "waymo311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
