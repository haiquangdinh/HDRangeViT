{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c379e83a",
   "metadata": {},
   "source": [
    "# <span style=\"color:red; font-weight:bold; \">A clean and modern RangeViT implementation for Waymo</span>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b73372e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from model.WaymoSegmentationDataset import WaymoSegmentationDataset\n",
    "from model.RangeViTSegmentationModel import RangeViTSegmentationModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ace32aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "val_dataset = WaymoSegmentationDataset('../WoD/validation', training=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f224fafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use torchmetrics or do manually\n",
    "from torchmetrics.classification import MulticlassJaccardIndex\n",
    "# create a metric and put it on gpu\n",
    "iou_metric = MulticlassJaccardIndex(num_classes=20, average=None, ignore_index=0).to(device)\n",
    "# Use torchmetrics to compute class-wise Precision and Recall\n",
    "from torchmetrics.classification import MulticlassPrecision, MulticlassRecall\n",
    "precision_metric = MulticlassPrecision(num_classes=20, average=None, ignore_index=0).to(device)\n",
    "recall_metric = MulticlassRecall(num_classes=20, average=None, ignore_index=0).to(device)\n",
    "\n",
    "num_classes = 20\n",
    "in_channels = 9 # range, x, y, z, intensity, flag, R, G, B\n",
    "model = RangeViTSegmentationModel(n_classes=num_classes, in_channels=in_channels).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dec6f1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, loader, iou_metric, precision_metric, recall_metric):\n",
    "    model.eval()\n",
    "    iou_metric.reset()  # Reset the IoU metric for the evaluation\n",
    "    precision_metric.reset()\n",
    "    recall_metric.reset()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch_bar = tqdm(loader, desc=\"Evaluating\", leave=False)\n",
    "        for imgs, labels in batch_bar:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            iou_metric.update(preds, labels)\n",
    "            precision_metric.update(preds, labels)\n",
    "            recall_metric.update(preds, labels)\n",
    "\n",
    "            ious = iou_metric.compute()\n",
    "            precisions = precision_metric.compute()\n",
    "            recalls = recall_metric.compute()\n",
    "            \n",
    "            mean_iou = torch.mean(ious[ious != 0])\n",
    "            mean_precision = torch.mean(precisions[precisions != 0])\n",
    "            mean_recall = torch.mean(recalls[recalls != 0])\n",
    "            batch_bar.set_postfix(mIoU=mean_iou.item())\n",
    "\n",
    "    print(f\"mIoU: {mean_iou.item():.4f}, mPrecision: {mean_precision.item():.4f}, mRecall: {mean_recall.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06a87603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained model from range_vit_waymo.pth\n"
     ]
    }
   ],
   "source": [
    "# Load the model if you have a pre-trained one\n",
    "pretrain_path = 'range_vit_waymo.pth'\n",
    "if os.path.exists(pretrain_path):\n",
    "    print(f\"Loading pre-trained model from {pretrain_path}\")\n",
    "    model.load_state_dict(torch.load(pretrain_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8b18f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "247c2bb7ba9a429d85d8ee342c8e6113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5976 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Encountered different devices in metric calculation (see stacktrace for details). This could be due to the metric class not being on the same device as input. Instead of `metric=MulticlassPrecision(...)` try to do `metric=MulticlassPrecision(...).to(device)` where device corresponds to the device of the input.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pykitti/lib/python3.13/site-packages/torchmetrics/metric.py:549\u001b[39m, in \u001b[36mMetric._wrap_update.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m     \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pykitti/lib/python3.13/site-packages/torchmetrics/classification/stat_scores.py:347\u001b[39m, in \u001b[36mMulticlassStatScores.update\u001b[39m\u001b[34m(self, preds, target)\u001b[39m\n\u001b[32m    344\u001b[39m tp, fp, tn, fn = _multiclass_stat_scores_update(\n\u001b[32m    345\u001b[39m     preds, target, num_classes, \u001b[38;5;28mself\u001b[39m.top_k, \u001b[38;5;28mself\u001b[39m.average, \u001b[38;5;28mself\u001b[39m.multidim_average, \u001b[38;5;28mself\u001b[39m.ignore_index\n\u001b[32m    346\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pykitti/lib/python3.13/site-packages/torchmetrics/classification/stat_scores.py:77\u001b[39m, in \u001b[36m_AbstractStatScores._update_state\u001b[39m\u001b[34m(self, tp, fp, tn, fn)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[38;5;28mself\u001b[39m.tp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtp\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mtp\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.tp, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [*\u001b[38;5;28mself\u001b[39m.tp, tp]\n\u001b[32m     78\u001b[39m     \u001b[38;5;28mself\u001b[39m.fp = \u001b[38;5;28mself\u001b[39m.fp + fp \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.fp, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [*\u001b[38;5;28mself\u001b[39m.fp, fp]\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43meval_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecision_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecall_metric\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36meval_model\u001b[39m\u001b[34m(model, loader, iou_metric, precision_metric, recall_metric)\u001b[39m\n\u001b[32m     14\u001b[39m preds = outputs.argmax(dim=\u001b[32m1\u001b[39m)\n\u001b[32m     15\u001b[39m iou_metric.update(preds, labels)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mprecision_metric\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m recall_metric.update(preds, labels)\n\u001b[32m     19\u001b[39m ious = iou_metric.compute()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pykitti/lib/python3.13/site-packages/torchmetrics/metric.py:552\u001b[39m, in \u001b[36mMetric._wrap_update.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    550\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    551\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mExpected all tensors to be on\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n\u001b[32m--> \u001b[39m\u001b[32m552\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    553\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mEncountered different devices in metric calculation (see stacktrace for details).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    554\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m This could be due to the metric class not being on the same device as input.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    555\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Instead of `metric=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m(...)` try to do\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    556\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m `metric=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m(...).to(device)` where\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    557\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m device corresponds to the device of the input.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    558\u001b[39m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    559\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    561\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_on_cpu:\n",
      "\u001b[31mRuntimeError\u001b[39m: Encountered different devices in metric calculation (see stacktrace for details). This could be due to the metric class not being on the same device as input. Instead of `metric=MulticlassPrecision(...)` try to do `metric=MulticlassPrecision(...).to(device)` where device corresponds to the device of the input."
     ]
    }
   ],
   "source": [
    "eval_model(model, val_loader, iou_metric, precision_metric, recall_metric)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pykitti",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
